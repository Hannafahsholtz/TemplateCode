# erin baggott
# july 29, 2016
# create country-day datasetfrom __future__ import division
import os, re, string, csv
rawdir = '/nfs/home/E/ebaggott/shared_space/ebaggott/fearonFeb2016/'
dir = '/nfs/home/E/ebaggott/shared_space/ebaggott/fearonFeb2016/'

decade = '201'
############## load docs ##############allclean = []alldates = []for file in os.listdir(rawdir):    if 'rawNYT_'+decade in file:        print file        f = open(rawdir+file,'r').read()        rawdocs = re.compile('\n\w{3,9} \d{2}, \d{4} - ').split(f)        rawdates = re.findall(r'\n\w{3,9} \d{2}, \d{4} - ',f)        # process dates        dates = []        for date in rawdates:            date = re.findall(r'\w{3,9} \d{1,2}, \d{4}',date)[0]            year = date.split(' ')[2]            month = date.split(' ')[0]            monthdic = {'January':1, 'February':2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7, 'August':8, 'September':9, 'October':10, 'November':11, 'December':12}            month = str(monthdic[month.title()])            if len(month)==1:                month = '0'+month            day = date.split(',')[0].split(' ')[1]            date = year+'-'+month+'-'+day            #print date            dates.append(date)        # process articles        clean = []        for doc in rawdocs:            if 'TimesMachine\n' in doc:                doc = doc.split('TimesMachine\n')[1]            # for post 1981 docs that do NOT have TimeMachine in text:            if 'Print Headline: ' in doc:                doc = doc.split('\n')                doc = ' '.join(doc[1:len(doc)]) # chop off last doc's description            doc = doc.lower()            if doc != '':                clean.append(doc)        # write out clean docs        print 'len clean = '+str(len(clean))        print 'len dates = '+str(len(dates))        allclean.extend(clean)        alldates.extend(dates)            ###################################### replace problematic country names ####################################### strip string punctuation, tags, numbers, chinese punctuationdef strip_punctuation(s):    return re.sub("([%s]+)" % string.punctuation, " ", s)def strip_tags(s):    # assumes s is already lowercase    return re.sub(r"<([^>]+)>", "", s)def strip_numeric(s):    return re.sub(r"[0-9]+", "", s)allclean2 = []for doc in allclean:    # UK    doc = doc.replace('britain','united kingdom')    doc = doc.replace('england','united kingdom')    doc = doc.replace('scotland','united kingdom')    doc = doc.replace('northern ireland','united kingdom')    # DPRK    doc = doc.replace('dprk','north_korea')    doc = doc.replace('d.p.r.k.','north_korea')    doc = doc.replace('d. p. r. k.','north_korea')    doc = doc.replace("democratic people's republic of korea",'north_korea')    # korea -- careful!    doc = doc.replace('republic of korea','south_korea')    doc = doc.replace(' korea',' south_korea')    # DRV    doc = doc.replace("democratic people's republic of vietnam",'drv')    # RVN    doc = doc.replace("republic of vietnam",'rvn')    # ROC    doc = doc.replace('republic of congo','roc')    # DRC    doc = doc.replace('democratic republic of congo','drc')    doc = doc.replace('congo','drc')    # USSR    doc = doc.replace('ussr','russia')    doc = doc.replace('u.s.s.r.','russia')    doc = doc.replace('u. s. s. r.','russia')    doc = doc.replace('soviet union','russia')    doc = doc.replace('soviet','russia')
# east germany      doc = doc.replace('east germany','east_germany')    doc = doc.replace(' gdr','east_germany')    doc = doc.replace('g.d.r.','east_germany')    doc = doc.replace('g. d. r.','east_germany')    doc = doc.replace('german democratic republic','east_germany') # old    # west germany    doc = doc.replace('west germany','west_germany')    doc = doc.replace(' gfr','west_germany')    doc = doc.replace('g.f.r.','west_germany')    doc = doc.replace('g. f. r.','west_germany')    doc = doc.replace('german federal republic','west_germany') # old    # process for split    doc = doc.replace('\n',' ')    doc = strip_punctuation(doc)    doc = strip_tags(doc)    doc = strip_numeric(doc)    allclean2.append(doc)allclean = []############# tokenize ############## load dictionaries, unitoken them, and replace in allclean2. then count!dicfile = dir+'COWcountryNameDictionary.csv'with open(dicfile,'rb') as f:    reader = csv.reader(f, delimiter=",")    next(reader,None)    cabb, ccode, tokens = zip(*reader)    tokensclean = []for token in tokens:    token = token.lower()    token = strip_punctuation(token)    tokensclean.append(token)    #print tokenunitokens = []for token in tokens:    token = token.lower()    token = strip_punctuation(token)    token = token.replace(' ','_')    unitokens.append(token)    #print token    tokdic = dict(zip(tokensclean,unitokens))tokenized = []for article in allclean2:    for i, entry in enumerate(tokdic):        #print entry # these are without spaces        if entry in article:            article = article.replace(tokdic.keys()[i],tokdic.values()[i])    tokenized.append(article)     allclean2 = []########################### count articles per day ############################ create date columnfrom datetime import date, datetime, timedeltadef perdelta(start, end, delta):    curr = start    while curr < end:        yield curr        curr += deltastart = int(2010)
end = int(2015)df = [str(day) for day in perdelta(date(start, 1, 1), date(end, 1, 1), timedelta(days=1))]print 'df: ',df[0],df[len(df)-1]     ################################## count country-day references ##################################
# start csvwith open(dir+'export/publicArchiveData2010-2014.csv','wb') as f:    file_writer = csv.writer(f)    header = ['token','date','refs','awm','awm_words','ta','twords']    file_writer.writerow(header)
# mod loop to write out day level data:with open(dir+'export/publicArchiveData2010-2014.csv','a') as f:    file_writer = csv.writer(f)    for i,token in enumerate(unitokens):        print token, i/len(unitokens), '% done'        for date in df:            #print date            refs = 0
            awm = 0
            awm_words = 0
            ta = 0
            twords = 0            docset = [doc for i,doc in enumerate(tokenized) if date==alldates[i]]            for doc in docset:                words = doc.split()
                newrefs = words.count(token)
                if newrefs >0:
                	awm += 1
                	awm_words += len(words)                refs += newrefs  
                ta += 1
                twords += len(words)            row = [token,date,refs,awm,awm_words,ta,twords]            file_writer.writerow(row)                         